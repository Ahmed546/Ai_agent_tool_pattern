{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_current_weather(location: str, unit: str):\n",
    "\t\"\"\"\n",
    "\tGet the current weather in a given location\n",
    "\n",
    "\tlocation (str): The city and state, e.g. Pakistan, Lahore\n",
    "\tunit (str): The unit. It can take two values; \"celsius\", \"fahrenheit\"\n",
    "\t\"\"\"\n",
    "\tif location == \"Madrid\":\n",
    "\t\treturn json.dumps({\"temperature\": 25, \"unit\": unit})\n",
    "\n",
    "\telse:\n",
    "\t\treturn json.dumps({\"temperature\": 58, \"unit\": unit})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] =  os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3-70b-8192\"\n",
    "GROQ_CLIENT = Groq()\n",
    "\n",
    "# Define the System Prompt as a constant\n",
    "TOOL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. \n",
    "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug \n",
    "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
    "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "\n",
    "<tool_call>\n",
    "{\"name\": <function-name>,\"arguments\": <args-dict>}\n",
    "</tool_call>\n",
    "\n",
    "Here are the available tools:\n",
    "\n",
    "<tools> {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Punjab, Lahore unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",\n",
    "    \"parameters\": {\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"str\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"str\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "</tools>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": TOOL_SYSTEM_PROMPT\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What's the current temperature in Lahore, in Celsius?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_chat_history.append(user_msg)\n",
    "agent_chat_history.append(user_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tool_call>\n",
      "{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Lahore, Punjab\", \"unit\": \"celsius\"}}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "output = GROQ_CLIENT.chat.completions.create(\n",
    "    messages=tool_chat_history,\n",
    "    model=MODEL\n",
    ").choices[0].message.content\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool_call_str(tool_call_str: str):\n",
    "    pattern = r'</?tool_call>'\n",
    "    clean_tags = re.sub(pattern, '', tool_call_str)\n",
    "    \n",
    "    try:\n",
    "        tool_call_json = json.loads(clean_tags)\n",
    "        return tool_call_json\n",
    "    except json.JSONDecodeError:\n",
    "        return clean_tags\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return \"There was some error parsing the Tool's output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_current_weather',\n",
       " 'arguments': {'location': 'Lahore, Punjab', 'unit': 'celsius'}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_output = parse_tool_call_str(output)\n",
    "parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"temperature\": 58, \"unit\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "result = get_current_weather(**parsed_output[\"arguments\"])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chat_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Observation: {result}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think there may be a mistake here. 58째C is extremely hot, even for Lahore. I'll check the current temperature in Lahore for you. According to current weather reports, the temperature in Lahore is around 25-30째C (77-86째F). Not 58째C!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GROQ_CLIENT.chat.completions.create(\n",
    "    messages=agent_chat_history,\n",
    "    model=MODEL\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from tool_pattern.tool import tool\n",
    "from tool_pattern.tool_agent import ToolAgent\n",
    "\n",
    "def fetch_top_hacker_news_stories(top_n: int):\n",
    "    \"\"\"\n",
    "    Fetch the top stories from Hacker News.\n",
    "\n",
    "    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API. \n",
    "    Each story contains the title, URL, score, author, and time of submission. The data is fetched \n",
    "    from the official Firebase Hacker News API, which returns story details in JSON format.\n",
    "\n",
    "    Args:\n",
    "        top_n (int): The number of top stories to retrieve.\n",
    "    \"\"\"\n",
    "    top_stories_url = 'https://hacker-news.firebaseio.com/v0/topstories.json'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(top_stories_url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        \n",
    "        # Get the top story IDs\n",
    "        top_story_ids = response.json()[:top_n]\n",
    "        \n",
    "        top_stories = []\n",
    "        \n",
    "        # For each story ID, fetch the story details\n",
    "        for story_id in top_story_ids:\n",
    "            story_url = f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json'\n",
    "            story_response = requests.get(story_url)\n",
    "            story_response.raise_for_status()  # Check for HTTP errors\n",
    "            story_data = story_response.json()\n",
    "            \n",
    "            # Append the story title and URL (or other relevant info) to the list\n",
    "            top_stories.append({\n",
    "                'title': story_data.get('title', 'No title'),\n",
    "                'url': story_data.get('url', 'No URL available'),\n",
    "            })\n",
    "        \n",
    "        return json.dumps(top_stories)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': \"You can't use your $6,299.00 Camera as a Webcam. That will be $5\",\n",
       "  'url': 'https://romanzipp.com/blog/no-you-cant-use-your-6299-canon-camera-as-a-webcam'},\n",
       " {'title': 'Thoughts on a Month with Devin',\n",
       "  'url': 'https://www.answer.ai/posts/2025-01-08-devin.html'},\n",
       " {'title': 'No Calls', 'url': 'https://keygen.sh/blog/no-calls/'},\n",
       " {'title': 'Starship Flight 7',\n",
       "  'url': 'https://www.spacex.com/launches/mission/?missionId=starship-flight-7?submit'},\n",
       " {'title': 'PostgreSQL Anonymizer',\n",
       "  'url': 'https://postgresql-anonymizer.readthedocs.io/en/stable/'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(fetch_top_hacker_news_stories(top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_tool = tool(fetch_top_hacker_news_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fetch_top_hacker_news_stories'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'fetch_top_hacker_news_stories',\n",
       " 'description': '\\n    Fetch the top stories from Hacker News.\\n\\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API. \\n    Each story contains the title, URL, score, author, and time of submission. The data is fetched \\n    from the official Firebase Hacker News API, which returns story details in JSON format.\\n\\n    Args:\\n        top_n (int): The number of top stories to retrieve.\\n    ',\n",
       " 'parameters': {'properties': {'top_n': {'type': 'int'}}}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(hn_tool.fn_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_agent = ToolAgent(tools=[hn_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\AI-Projects\\Ai_agent_tool_pattern\\trails.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/AI-Projects/Ai_agent_tool_pattern/trails.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m tool_agent\u001b[39m.\u001b[39mrun(user_msg\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTell me your name\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\AI-Projects\\Ai_agent_tool_pattern\\tool_pattern\\tool_agent.py:128\u001b[0m, in \u001b[0;36mToolAgent.run\u001b[1;34m(self, user_msg)\u001b[0m\n\u001b[0;32m    117\u001b[0m tool_chat_history \u001b[39m=\u001b[39m ChatHistory(\n\u001b[0;32m    118\u001b[0m     [\n\u001b[0;32m    119\u001b[0m         build_prompt_structure(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     ]\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    126\u001b[0m agent_chat_history \u001b[39m=\u001b[39m ChatHistory([user_prompt])\n\u001b[1;32m--> 128\u001b[0m tool_call_response \u001b[39m=\u001b[39m completions_create(\n\u001b[0;32m    129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient, messages\u001b[39m=\u001b[39mtool_chat_history, model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    131\u001b[0m tool_calls \u001b[39m=\u001b[39m extract_tag_content(\u001b[39mstr\u001b[39m(tool_call_response), \u001b[39m\"\u001b[39m\u001b[39mtool_call\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m tool_calls\u001b[39m.\u001b[39mfound:\n",
      "File \u001b[1;32md:\\AI-Projects\\Ai_agent_tool_pattern\\utils\\completions.py:13\u001b[0m, in \u001b[0;36mcompletions_create\u001b[1;34m(client, messages, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompletions_create\u001b[39m(client, messages: \u001b[39mlist\u001b[39m, model: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Sends a request to the client's `completions.create` method to interact with the language model.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m        str: The content of the model's response.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(messages\u001b[39m=\u001b[39mmessages, model\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[1;32md:\\AI-Projects\\Ai_agent_tool_pattern\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:316\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    164\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    165\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    195\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    196\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[0;32m    317\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/openai/v1/chat/completions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    318\u001b[0m         body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[0;32m    319\u001b[0m             {\n\u001b[0;32m    320\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages,\n\u001b[0;32m    321\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[0;32m    322\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    323\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m: function_call,\n\u001b[0;32m    324\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunctions\u001b[39m\u001b[39m\"\u001b[39m: functions,\n\u001b[0;32m    325\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogit_bias\u001b[39m\u001b[39m\"\u001b[39m: logit_bias,\n\u001b[0;32m    326\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m: logprobs,\n\u001b[0;32m    327\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_completion_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    328\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_tokens,\n\u001b[0;32m    329\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[0;32m    330\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mparallel_tool_calls\u001b[39m\u001b[39m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpresence_penalty\u001b[39m\u001b[39m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mresponse_format\u001b[39m\u001b[39m\"\u001b[39m: response_format,\n\u001b[0;32m    333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m: seed,\n\u001b[0;32m    334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mservice_tier\u001b[39m\u001b[39m\"\u001b[39m: service_tier,\n\u001b[0;32m    335\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[0;32m    336\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream,\n\u001b[0;32m    337\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[0;32m    338\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtool_choice\u001b[39m\u001b[39m\"\u001b[39m: tool_choice,\n\u001b[0;32m    339\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m: tools,\n\u001b[0;32m    340\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_logprobs\u001b[39m\u001b[39m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    341\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[0;32m    342\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: user,\n\u001b[0;32m    343\u001b[0m             },\n\u001b[0;32m    344\u001b[0m             completion_create_params\u001b[39m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    345\u001b[0m         ),\n\u001b[0;32m    346\u001b[0m         options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[0;32m    347\u001b[0m             extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[0;32m    348\u001b[0m         ),\n\u001b[0;32m    349\u001b[0m         cast_to\u001b[39m=\u001b[39mChatCompletion,\n\u001b[0;32m    350\u001b[0m         stream\u001b[39m=\u001b[39mstream \u001b[39mor\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    351\u001b[0m         stream_cls\u001b[39m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    352\u001b[0m     )\n",
      "File \u001b[1;32md:\\AI-Projects\\Ai_agent_tool_pattern\\venv\\Lib\\site-packages\\groq\\_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1254\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1262\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1263\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1264\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1265\u001b[0m     )\n\u001b[1;32m-> 1266\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32md:\\AI-Projects\\Ai_agent_tool_pattern\\venv\\Lib\\site-packages\\groq\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    956\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 958\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[0;32m    959\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m    960\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m    961\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[0;32m    962\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[0;32m    963\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[0;32m    964\u001b[0m )\n",
      "File \u001b[1;32md:\\AI-Projects\\Ai_agent_tool_pattern\\venv\\Lib\\site-packages\\groq\\_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m   1060\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1061\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[0;32m   1064\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m   1065\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[0;32m   1070\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "output = tool_agent.run(user_msg=\"Tell me your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
